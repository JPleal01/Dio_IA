{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO6qqLMFls36+gE6zpOHcr0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JPleal01/Dio_IA/blob/main/Rede_recomenda%C3%A7%C3%A3o.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25294c98"
      },
      "source": [
        "!pip install kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile kaggle.json\n",
        "{\"username\":\"<your kaggle username>\",\"key\":\"<your kaggle api key>\"}"
      ],
      "metadata": {
        "id": "6IsPK8Rg6Zs-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f98467d4"
      },
      "source": [
        "import os\n",
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97d7b42b"
      },
      "source": [
        "!kaggle datasets download -d paramaggarwal/fashion-product-images-small\n",
        "\n",
        "!unzip -q fashion-product-images-small.zip -d fashion-product-images-small"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61b5e879"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Define the path to the downloaded dataset\n",
        "dataset_path = 'fashion-product-images-small'\n",
        "styles_path = os.path.join(dataset_path, 'styles.csv')\n",
        "images_dir = os.path.join(dataset_path, 'images')\n",
        "\n",
        "# Load the styles.csv file to get image information and categories\n",
        "styles_df = pd.read_csv(styles_path, on_bad_lines='skip')\n",
        "\n",
        "# Select 4 subcategories to use for the recommendation program\n",
        "# Let's choose some common subcategories from the dataset\n",
        "selected_subcategories = ['Topwear', 'Bottomwear', 'Shoes', 'Dress']\n",
        "\n",
        "# Filter the dataframe to include only the selected subcategories\n",
        "filtered_df = styles_df[styles_df['subCategory'].isin(selected_subcategories)]\n",
        "\n",
        "# *** Add print statement to check filtered_df ***\n",
        "print(\"Shape of filtered_df:\", filtered_df.shape)\n",
        "print(\"First 5 rows of filtered_df:\")\n",
        "display(filtered_df.head())\n",
        "\n",
        "# Initialize lists\n",
        "image_file_paths = []\n",
        "labels = []\n",
        "preprocessed_images = []\n",
        "\n",
        "# Loop through the filtered dataframe to get image paths and labels\n",
        "for index, row in filtered_df.iterrows():\n",
        "    image_id = row['id']\n",
        "    subcategory = row['subCategory']\n",
        "    # Construct the image path\n",
        "    image_path = os.path.join(images_dir, f'{image_id}.jpg') # Assuming images are in JPG format\n",
        "\n",
        "    # Check if the image file exists before processing\n",
        "    if os.path.exists(image_path):\n",
        "        image_file_paths.append(image_path)\n",
        "        labels.append(subcategory)\n",
        "    else:\n",
        "        # Print a message if the image is not found\n",
        "        print(f\"Image not found: {image_path}\")\n",
        "\n",
        "\n",
        "# 4. Loop through each image file path\n",
        "for image_path in image_file_paths:\n",
        "    try:\n",
        "        # Load the image\n",
        "        img = Image.open(image_path).convert('RGB') # Ensure image is in RGB format\n",
        "\n",
        "        # Resize the image\n",
        "        img = img.resize((224, 224))\n",
        "\n",
        "        # Normalize the pixel values\n",
        "        img_array = np.array(img).astype('float32') / 255.0 # Normalize to [0, 1]\n",
        "\n",
        "        # Append the preprocessed image\n",
        "        preprocessed_images.append(img_array)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading or processing image {image_path}: {e}\")\n",
        "        # Remove the corresponding label if the image cannot be processed\n",
        "        # Find the index of the current image_path in the original list\n",
        "        try:\n",
        "            idx_to_remove = image_file_paths.index(image_path)\n",
        "            labels.pop(idx_to_remove)\n",
        "            image_file_paths.pop(idx_to_remove) # Also remove the path from the list being iterated\n",
        "        except ValueError:\n",
        "            # This case should ideally not happen if image_path is from image_file_paths\n",
        "            print(f\"Warning: Could not find {image_path} in the list to remove its label.\")\n",
        "\n",
        "\n",
        "# 5. Convert lists to NumPy arrays\n",
        "preprocessed_images_np = np.array(preprocessed_images)\n",
        "labels_np = np.array(labels)\n",
        "\n",
        "# Print shapes to verify\n",
        "print(\"Shape of preprocessed images:\", preprocessed_images_np.shape)\n",
        "print(\"Shape of labels:\", labels_np.shape)\n",
        "\n",
        "# You can also print the unique labels to confirm the selected categories are present\n",
        "print(\"Unique labels (subcategories):\", np.unique(labels_np))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52add5ff"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "dataset_path = 'fashion-product-images-small'\n",
        "styles_path = os.path.join(dataset_path, 'styles.csv')\n",
        "styles_df = pd.read_csv(styles_path, on_bad_lines='skip')\n",
        "\n",
        "# Print unique values in the 'subCategory' column\n",
        "print(\"Unique values in 'subCategory' column:\")\n",
        "print(styles_df['subCategory'].unique())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "440d175c"
      },
      "source": [
        "'''# 6. Split the preprocessed data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    preprocessed_images_np, labels_np, test_size=0.2, random_state=42, stratify=labels_np)\n",
        "\n",
        "# Print shapes of the splits to verify\n",
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "print(\"Shape of X_test:\", X_test.shape)\n",
        "print(\"Shape of y_train:\", y_train.shape)\n",
        "print(\"Shape of y_test:\", y_test.shape)'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2115d70f"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# 1. Define the input shape based on the preprocessed images\n",
        "input_shape = (224, 224, 3)\n",
        "input_tensor = Input(shape=input_shape)\n",
        "\n",
        "# 2. Load the pre-trained ResNet50 model, excluding the top classification layer\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_tensor=input_tensor)\n",
        "\n",
        "# 3. Freeze the layers of the base model\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Create a new model with the base model as the feature extractor\n",
        "feature_extractor = Model(inputs=base_model.input, outputs=base_model.output)\n",
        "\n",
        "# Print the model summary to verify the architecture and frozen layers\n",
        "feature_extractor.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "168e7f0c"
      },
      "source": [
        "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow as tf\n",
        "\n",
        "# 2. Get the output from the feature_extractor model.\n",
        "x = feature_extractor.output\n",
        "\n",
        "# 3. Add new layers on top of the frozen base model's output.\n",
        "x = Flatten()(x)\n",
        "x = Dense(256, activation='relu')(x) # Example: Adding a dense layer with 256 units and ReLU activation\n",
        "x = Dropout(0.5)(x) # Example: Adding dropout for regularization\n",
        "# The final Dense layer should have an output size equal to the number of product categories\n",
        "num_product_categories = len(np.unique(y_train))\n",
        "predictions = Dense(num_product_categories, activation='softmax')(x)\n",
        "\n",
        "# 4. Create the final model by specifying the input from the feature_extractor model's input and the output from the newly added layers.\n",
        "model = Model(inputs=feature_extractor.input, outputs=predictions)\n",
        "\n",
        "# 5. Compile the model with an appropriate optimizer, loss function, and metrics.\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary to verify the new layers\n",
        "model.summary()\n",
        "\n",
        "# 6. Prepare the labels for training.\n",
        "# Convert the string labels (y_train and y_test) into one-hot encoded format\n",
        "y_train_one_hot = tf.keras.utils.to_categorical(y_train, num_classes=num_product_categories)\n",
        "y_test_one_hot = tf.keras.utils.to_categorical(y_test, num_classes=num_product_categories)\n",
        "\n",
        "# Need to map string labels to integers first for one-hot encoding\n",
        "label_to_int = {label: i for i, label in enumerate(np.unique(y_train))}\n",
        "y_train_int = np.array([label_to_int[label] for label in y_train])\n",
        "y_test_int = np.array([label_to_int[label] for label in y_test])\n",
        "\n",
        "y_train_one_hot = tf.keras.utils.to_categorical(y_train_int, num_classes=num_product_categories)\n",
        "y_test_one_hot = tf.keras.utils.to_categorical(y_test_int, num_classes=num_product_categories)\n",
        "\n",
        "# 7. Train the fine-tuned model.\n",
        "epochs = 10 # Define the number of epochs\n",
        "batch_size = 32 # Define the batch size\n",
        "\n",
        "history = model.fit(X_train, y_train_one_hot,\n",
        "                    epochs=epochs,\n",
        "                    batch_size=batch_size,\n",
        "                    validation_data=(X_test, y_test_one_hot))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0L7effP0GJY"
      },
      "source": [
        "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow as tf\n",
        "import numpy as np # Ensure numpy is imported if not already\n",
        "\n",
        "# 2. Get the output from the feature_extractor model.\n",
        "x = feature_extractor.output\n",
        "\n",
        "# 3. Add new layers on top of the frozen base model's output.\n",
        "x = Flatten()(x)\n",
        "x = Dense(256, activation='relu')(x) # Example: Adding a dense layer with 256 units and ReLU activation\n",
        "x = Dropout(0.5)(x) # Example: Adding dropout for regularization\n",
        "# The final Dense layer should have an output size equal to the number of product categories\n",
        "num_product_categories = len(np.unique(y_train))\n",
        "predictions = Dense(num_product_categories, activation='softmax')(x)\n",
        "\n",
        "# 4. Create the final model by specifying the input from the feature_extractor model's input and the output from the newly added layers.\n",
        "model = Model(inputs=feature_extractor.input, outputs=predictions)\n",
        "\n",
        "# 5. Compile the model with an appropriate optimizer, loss function, and metrics.\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary to verify the new layers\n",
        "model.summary()\n",
        "\n",
        "# 6. Prepare the labels for training.\n",
        "# Need to map string labels to integers first for one-hot encoding\n",
        "label_to_int = {label: i for i, label in enumerate(np.unique(y_train))}\n",
        "y_train_int = np.array([label_to_int[label] for label in y_train])\n",
        "y_test_int = np.array([label_to_int[label] for label in y_test])\n",
        "\n",
        "# Convert the integer labels into one-hot encoded format\n",
        "y_train_one_hot = tf.keras.utils.to_categorical(y_train_int, num_classes=num_product_categories)\n",
        "y_test_one_hot = tf.keras.utils.to_categorical(y_test_int, num_classes=num_product_categories)\n",
        "\n",
        "# 7. Train the fine-tuned model.\n",
        "epochs = 10 # Define the number of epochs\n",
        "batch_size = 32 # Define the batch size\n",
        "\n",
        "history = model.fit(X_train, y_train_one_hot,\n",
        "                    epochs=epochs,\n",
        "                    batch_size=batch_size,\n",
        "                    validation_data=(X_test, y_test_one_hot))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07ba895a"
      },
      "source": [
        "# 1. Create a new model for feature extraction\n",
        "# Access the layer just before the final classification layer\n",
        "feature_extraction_model = Model(inputs=model.input, outputs=model.layers[-2].output)\n",
        "\n",
        "# 2. Use the feature extraction model to predict feature vectors for all preprocessed images\n",
        "# Combine X_train and X_test for extracting features from the full dataset\n",
        "all_images = np.concatenate((X_train, X_test), axis=0)\n",
        "all_labels = np.concatenate((y_train, y_test), axis=0) # Also combine labels for storage\n",
        "\n",
        "image_features = feature_extraction_model.predict(all_images)\n",
        "\n",
        "# Print shapes to verify\n",
        "print(\"Shape of extracted features:\", image_features.shape)\n",
        "print(\"Shape of corresponding labels:\", all_labels.shape)\n",
        "\n",
        "# 3. Store the extracted feature vectors and their corresponding labels\n",
        "# You can store them as NumPy arrays or in a pandas DataFrame\n",
        "# Storing as NumPy arrays is straightforward here\n",
        "stored_features = image_features\n",
        "stored_labels = all_labels\n",
        "\n",
        "# Display the shapes to confirm they are stored\n",
        "print(\"Shape of stored features:\", stored_features.shape)\n",
        "print(\"Shape of stored labels:\", stored_labels.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7c1d3fef"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Calculate the pairwise cosine similarity matrix\n",
        "similarity_matrix = cosine_similarity(stored_features)\n",
        "\n",
        "# Print the shape of the similarity matrix to verify\n",
        "print(\"Shape of similarity matrix:\", similarity_matrix.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36cb59b4"
      },
      "source": [
        "# Store the calculated similarity matrix\n",
        "# It is already stored in the variable `similarity_matrix`\n",
        "# No explicit saving to a file is requested in this subtask, so keeping it in memory is sufficient.\n",
        "\n",
        "# Verify the variable exists\n",
        "print(\"Similarity matrix is stored in the variable 'similarity_matrix'.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "879a46bd"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def recommend_products(query_product_index, n_recommendations):\n",
        "    \"\"\"\n",
        "    Recommends similar products based on a query product's index.\n",
        "\n",
        "    Args:\n",
        "        query_product_index: The index of the query product in the stored_features array.\n",
        "        n_recommendations: The number of recommendations desired.\n",
        "\n",
        "    Returns:\n",
        "        A list of recommended product labels.\n",
        "    \"\"\"\n",
        "    # 2. Retrieve the similarity scores for the query product\n",
        "    query_similarity_scores = similarity_matrix[query_product_index]\n",
        "\n",
        "    # 3. Get the indices of the top 'n' most similar products (excluding the query product itself)\n",
        "    # Use argsort to get indices that would sort the array in descending order\n",
        "    sorted_indices = np.argsort(query_similarity_scores)[::-1]\n",
        "\n",
        "    # Exclude the query product itself and get the top n recommendations\n",
        "    # Slice the sorted indices to get the top n+1 (including the query product)\n",
        "    top_n_indices = sorted_indices[1:n_recommendations + 1]\n",
        "\n",
        "    # 4. Use these indices to retrieve the labels of the recommended products\n",
        "    recommended_product_labels = [stored_labels[i] for i in top_n_indices]\n",
        "\n",
        "    # 5. Return a list of the recommended product labels\n",
        "    return recommended_product_labels\n",
        "\n",
        "# 6. Test the function with a sample query product index and number of recommendations\n",
        "sample_query_index = 0  # Example: Use the first product as the query\n",
        "num_recommendations = 3 # Example: Request 3 recommendations\n",
        "\n",
        "recommended_items = recommend_products(sample_query_index, num_recommendations)\n",
        "print(f\"Recommended products for product at index {sample_query_index}: {recommended_items}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}